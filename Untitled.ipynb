{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-993bb1b430da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tflearn'"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import pandas\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tflearn\n",
    "import numpy\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from tflearn.layers.conv import conv_2d\n",
    "\n",
    "# Data loading\n",
    "from tflearn.datasets import cifar10\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# filter_number = int(sys.argv[1])\n",
    "# filter_size = int(sys.argv[2])\n",
    "\n",
    "\n",
    "filter_number = 16\n",
    "filter_size = 4\n",
    "\n",
    "is_turn = 0\n",
    "turn = 0\n",
    "\n",
    "snr = sys.argv[1]\n",
    "epoch = int(sys.argv[2])\n",
    "# turn = sys.argv[2]\n",
    "# is_turn = 1\n",
    "file_name = \"dataset-awgn-conv-\" + str(snr) + \"db-pre-raw.csv\"\n",
    "# file_name = \"dataset-awgn-conv--10db-pre-raw.csv\"\n",
    "\n",
    "classes = 15\n",
    "shape = [32, 32, 4]\n",
    "pretreatment = 3\n",
    "\n",
    "\n",
    "def get_dataset(matrix):\n",
    "    dataset = []\n",
    "    for s in matrix:\n",
    "        a = s.strip().split(' ')\n",
    "        col = []\n",
    "        for b in a:\n",
    "            row = []\n",
    "            b = float(b)\n",
    "            row.append(b)\n",
    "            col.append(row)\n",
    "        if pretreatment == 1:\n",
    "            # gfft\n",
    "            col.append([0, 0])\n",
    "        elif pretreatment == 2:\n",
    "            # llr cal\n",
    "            col.append([0])\n",
    "        elif pretreatment == 3:\n",
    "            # llr raw\n",
    "            i = 96\n",
    "            for j in range(i):\n",
    "                col.append([0])\n",
    "        dataset.append(numpy.array([col]).reshape((shape[0], shape[1], shape[2])))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataframe = pandas.read_csv(file_name)\n",
    "X = numpy.array(get_dataset(dataframe['X']))\n",
    "Y = dataframe['Y']\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(X, Y)\n",
    "x_train = X\n",
    "y_train = Y\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_valid_class = y_valid\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)\n",
    "y_valid_num = y_valid\n",
    "\n",
    "# X = numpy.expand_dims(X, axis=0)\n",
    "# X = numpy.expand_dims(X, axis=0)\n",
    "# testX = numpy.expand_dims(testX, axis=0)\n",
    "# testX = numpy.expand_dims(testX, axis=\n",
    "\n",
    "# Add noise\n",
    "# X = X + numpy.random.random((50000, 32, 32, 3)) * 0.1\n",
    "# testX = testX + numpy.random.random((10000, 32, 32, 3)) * 0.1\n",
    "\n",
    "# Transform labels to one-hot format\n",
    "# Y = tflearn.data_utils.to_categorical(Y, 10)\n",
    "# testY = tflearn.data_utils.to_categorical(testY, 10)\n",
    "y_train = tflearn.data_utils.to_categorical(y_train, classes)\n",
    "y_valid = tflearn.data_utils.to_categorical(y_valid, classes)\n",
    "\n",
    "\n",
    "def residual_shrinkage_block(incoming, nb_blocks, out_channels, downsample=False,\n",
    "                             downsample_strides=2, activation='relu', batch_norm=True,\n",
    "                             bias=True, weights_init='variance_scaling',\n",
    "                             bias_init='zeros', regularizer='L2', weight_decay=0.0001,\n",
    "                             trainable=True, restore=True, reuse=False, scope=None,\n",
    "                             name=\"ResidualBlock\", filter_size_residual=3):\n",
    "    # residual shrinkage blocks with channel-wise thresholds\n",
    "\n",
    "    residual = incoming\n",
    "    in_channels = incoming.get_shape().as_list()[-1]\n",
    "\n",
    "    # Variable Scope fix for older TF\n",
    "    try:\n",
    "        vscope = tf.variable_scope(scope, default_name=name, values=[incoming], reuse=reuse)\n",
    "    except Exception:\n",
    "        vscope = tf.variable_op_scope([incoming], scope, name, reuse=reuse)\n",
    "\n",
    "    with vscope as scope:\n",
    "        name = scope.name  # TODO\n",
    "\n",
    "        for i in range(nb_blocks):\n",
    "\n",
    "            identity = residual\n",
    "\n",
    "            if not downsample:\n",
    "                downsample_strides = 1\n",
    "\n",
    "            if batch_norm:\n",
    "                residual = tflearn.batch_normalization(residual)\n",
    "\n",
    "            residual = tflearn.activation(residual, activation)\n",
    "            residual = conv_2d(residual, out_channels, filter_size_residual,\n",
    "                               downsample_strides, 'same', 'linear',\n",
    "                               bias, weights_init, bias_init,\n",
    "                               regularizer, weight_decay, trainable,\n",
    "                               restore)\n",
    "\n",
    "            if batch_norm:\n",
    "                residual = tflearn.batch_normalization(residual)\n",
    "            residual = tflearn.activation(residual, activation)\n",
    "            residual = conv_2d(residual, out_channels, filter_size_residual, 1, 'same',\n",
    "                               'linear', bias, weights_init,\n",
    "                               bias_init, regularizer, weight_decay,\n",
    "                               trainable, restore)\n",
    "\n",
    "            # get thresholds and apply thresholding\n",
    "            abs_mean = tf.reduce_mean(tf.reduce_mean(tf.abs(residual), axis=2, keep_dims=True), axis=1, keep_dims=True)\n",
    "            scales = tflearn.fully_connected(abs_mean, n_units=out_channels // 4, activation='linear', regularizer='L2',\n",
    "                                             weight_decay=0.0001, weights_init='variance_scaling')\n",
    "            scales = tflearn.batch_normalization(scales)\n",
    "            scales = tflearn.activation(scales, 'relu')\n",
    "            scales = tflearn.fully_connected(scales, n_units=out_channels, activation='linear', regularizer='L2',\n",
    "                                             weight_decay=0.0001, weights_init='variance_scaling')\n",
    "            scales = tf.expand_dims(tf.expand_dims(scales, axis=1), axis=1)\n",
    "            thres = tf.multiply(abs_mean, tflearn.activations.sigmoid(scales))\n",
    "            # soft thresholding\n",
    "            residual = tf.multiply(tf.sign(residual), tf.maximum(tf.abs(residual) - thres, 0))\n",
    "\n",
    "            # Downsampling\n",
    "            if downsample_strides > 1:\n",
    "                identity = tflearn.avg_pool_2d(identity, 1,\n",
    "                                               downsample_strides)\n",
    "\n",
    "            # Projection to new dimension\n",
    "            if in_channels != out_channels:\n",
    "                if (out_channels - in_channels) % 2 == 0:\n",
    "                    ch = (out_channels - in_channels) // 2\n",
    "                    identity = tf.pad(identity,\n",
    "                                      [[0, 0], [0, 0], [0, 0], [ch, ch]])\n",
    "                else:\n",
    "                    ch = (out_channels - in_channels) // 2\n",
    "                    identity = tf.pad(identity,\n",
    "                                      [[0, 0], [0, 0], [0, 0], [ch, ch + 1]])\n",
    "                in_channels = out_channels\n",
    "\n",
    "            residual = residual + identity\n",
    "\n",
    "    return residual\n",
    "\n",
    "\n",
    "# Real-time data preprocessing\n",
    "img_prep = tflearn.ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center(per_channel=True)\n",
    "\n",
    "# Real-time data augmentation\n",
    "img_aug = tflearn.ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "\n",
    "# Build a Deep Residual Shrinkage Network with 3 blocks\n",
    "'''\n",
    "# Real-time data augmentation\n",
    "img_aug.add_random_crop([32, 32], padding=4)\n",
    "\n",
    "# Build a Deep Residual Shrinkage Network with 3 blocks\n",
    "net = tflearn.input_data(shape=[None, 32, 32, 3],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "net = tflearn.conv_2d(net, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "net = residual_shrinkage_block(net, 1, 16)\n",
    "net = residual_shrinkage_block(net, 1, 32, downsample=True)\n",
    "net = residual_shrinkage_block(net, 1, 32, downsample=True)\n",
    "\n",
    "nb_filter: `int`. The number of convolutional filters.\n",
    "filter_size: `int` or `list of int`. Size of filters.\n",
    "\n",
    "n_units: 神经元个数\n",
    "'''\n",
    "\n",
    "img_aug.add_random_crop([shape[0], shape[1]], padding=4)\n",
    "\n",
    "net = tflearn.input_data(shape=[None, shape[0], shape[1], shape[2]],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "net = tflearn.conv_2d(net, nb_filter=filter_number, filter_size=filter_size, regularizer='L2', weight_decay=0.0001)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number,\n",
    "                               filter_size_residual=filter_size)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number * 2,\n",
    "                               filter_size_residual=filter_size, downsample=True)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number * 2,\n",
    "                               filter_size_residual=filter_size, downsample=True)\n",
    "\n",
    "net = tflearn.batch_normalization(net)\n",
    "net = tflearn.activation(net, 'relu')\n",
    "net = tflearn.global_avg_pool(net)\n",
    "\n",
    "# Regression\n",
    "net = tflearn.fully_connected(net, n_units=classes, activation='softmax')\n",
    "mom = tflearn.Momentum(learning_rate=0.1, lr_decay=0.1, decay_step=20000, staircase=True)\n",
    "net = tflearn.regression(net, optimizer=mom, loss='categorical_crossentropy')\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(net, checkpoint_path='model-channel-coding', max_checkpoints=10,\n",
    "                    tensorboard_verbose=0, clip_gradients=0.)\n",
    "\n",
    "# epoch = 400\n",
    "\n",
    "model.fit(x_train, y_train, n_epoch=epoch, snapshot_epoch=False, snapshot_step=2000, show_metric=True,\n",
    "          batch_size=50, shuffle=True, run_id='model-channel-coding')\n",
    "\n",
    "\n",
    "def get_acc(epoch_read):\n",
    "    training_acc = model.evaluate(x_train, y_train)\n",
    "    validation_acc = model.evaluate(x_valid, y_valid)\n",
    "    print(\"\\n\")\n",
    "    print(\"tranining_acc = {}\".format(training_acc))\n",
    "    print(\"validation_acc = {}\".format(validation_acc))\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    for i in range(len(y_pred)):\n",
    "        max_value = max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value == y_pred[i][j]:\n",
    "                y_pred[i][j] = 1\n",
    "            else:\n",
    "                y_pred[i][j] = 0\n",
    "\n",
    "    y_pred_num = []\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_valid)):\n",
    "            if all(y_pred[i] == y_valid[j]):\n",
    "                y_pred_num.append(y_valid_num[j])\n",
    "                break\n",
    "\n",
    "    y_pred_class = encoder.inverse_transform(y_pred_num)\n",
    "    validation_report = classification_report(y_valid_class, y_pred_class, digits=8)\n",
    "    print(validation_report)\n",
    "\n",
    "    with open(\"{}.txt\".format(datetime.datetime.now().strftime('%Y-%m-%d %H-%M-%S.%f')), \"w\") as f:\n",
    "        f.write(\"filter_number = {}\\n\".format(filter_number))\n",
    "        f.write(\"filter_size = {}\\n\".format(filter_size))\n",
    "        f.write(\"file_name = {}\\n\".format(file_name))\n",
    "        f.write(\"modulate = {}\\n\".format('bpsk'))\n",
    "        f.write(\"epoch = {}\\n\".format(epoch_read))\n",
    "        if is_turn:\n",
    "            f.write(\"turn = {}\\n\".format(turn))\n",
    "        f.write(\"training_acc = {}\\n\".format(training_acc))\n",
    "        f.write(\"validation_acc = {}\\n\".format(validation_acc))\n",
    "        f.write(validation_report)\n",
    "\n",
    "\n",
    "get_acc(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tflearn\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e7/3c/0b156d08ef3d4e2a8009ecab2af1ad2e304f6fb99562b6271c68a74a4397/tflearn-0.5.0.tar.gz (107 kB)\n",
      "Requirement already satisfied: numpy in d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages (from tflearn) (1.19.2)\n",
      "Requirement already satisfied: six in d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages (from tflearn) (1.15.0)\n",
      "Requirement already satisfied: Pillow in d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages (from tflearn) (8.0.1)\n",
      "Building wheels for collected packages: tflearn\n",
      "  Building wheel for tflearn (setup.py): started\n",
      "  Building wheel for tflearn (setup.py): finished with status 'done'\n",
      "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=68a46317735964330d00cefc1096c3125e42f0a642f97e1afc3f5ee3f18b3f3f\n",
      "  Stored in directory: c:\\users\\administrator\\appdata\\local\\pip\\cache\\wheels\\83\\6a\\e1\\4c36c4f320d691e3187a1b2607214262f8991f0e4e6d120c6d\n",
      "Successfully built tflearn\n",
      "Installing collected packages: tflearn\n",
      "Successfully installed tflearn-0.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for decorator: [Errno 2] No such file or directory: 'd:\\\\soft\\\\anaconda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\decorator-5.0.9.dist-info\\\\METADATA'\n"
     ]
    }
   ],
   "source": [
    "!pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-f59e3acd-9a59-4645-b654-68dbb9c4f3b0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-993bb1b430da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0msnr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;31m# turn = sys.argv[2]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# is_turn = 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-f59e3acd-9a59-4645-b654-68dbb9c4f3b0.json'"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import pandas\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tflearn\n",
    "import numpy\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from tflearn.layers.conv import conv_2d\n",
    "\n",
    "# Data loading\n",
    "from tflearn.datasets import cifar10\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# filter_number = int(sys.argv[1])\n",
    "# filter_size = int(sys.argv[2])\n",
    "\n",
    "\n",
    "filter_number = 16\n",
    "filter_size = 4\n",
    "\n",
    "is_turn = 0\n",
    "turn = 0\n",
    "\n",
    "snr = sys.argv[1]\n",
    "epoch = int(sys.argv[2])\n",
    "# turn = sys.argv[2]\n",
    "# is_turn = 1\n",
    "file_name = \"dataset-awgn-conv-\" + str(snr) + \"db-pre-raw.csv\"\n",
    "# file_name = \"dataset-awgn-conv--10db-pre-raw.csv\"\n",
    "\n",
    "classes = 15\n",
    "shape = [32, 32, 4]\n",
    "pretreatment = 3\n",
    "\n",
    "\n",
    "def get_dataset(matrix):\n",
    "    dataset = []\n",
    "    for s in matrix:\n",
    "        a = s.strip().split(' ')\n",
    "        col = []\n",
    "        for b in a:\n",
    "            row = []\n",
    "            b = float(b)\n",
    "            row.append(b)\n",
    "            col.append(row)\n",
    "        if pretreatment == 1:\n",
    "            # gfft\n",
    "            col.append([0, 0])\n",
    "        elif pretreatment == 2:\n",
    "            # llr cal\n",
    "            col.append([0])\n",
    "        elif pretreatment == 3:\n",
    "            # llr raw\n",
    "            i = 96\n",
    "            for j in range(i):\n",
    "                col.append([0])\n",
    "        dataset.append(numpy.array([col]).reshape((shape[0], shape[1], shape[2])))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataframe = pandas.read_csv(file_name)\n",
    "X = numpy.array(get_dataset(dataframe['X']))\n",
    "Y = dataframe['Y']\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(X, Y)\n",
    "x_train = X\n",
    "y_train = Y\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_valid_class = y_valid\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)\n",
    "y_valid_num = y_valid\n",
    "\n",
    "# X = numpy.expand_dims(X, axis=0)\n",
    "# X = numpy.expand_dims(X, axis=0)\n",
    "# testX = numpy.expand_dims(testX, axis=0)\n",
    "# testX = numpy.expand_dims(testX, axis=\n",
    "\n",
    "# Add noise\n",
    "# X = X + numpy.random.random((50000, 32, 32, 3)) * 0.1\n",
    "# testX = testX + numpy.random.random((10000, 32, 32, 3)) * 0.1\n",
    "\n",
    "# Transform labels to one-hot format\n",
    "# Y = tflearn.data_utils.to_categorical(Y, 10)\n",
    "# testY = tflearn.data_utils.to_categorical(testY, 10)\n",
    "y_train = tflearn.data_utils.to_categorical(y_train, classes)\n",
    "y_valid = tflearn.data_utils.to_categorical(y_valid, classes)\n",
    "\n",
    "\n",
    "def residual_shrinkage_block(incoming, nb_blocks, out_channels, downsample=False,\n",
    "                             downsample_strides=2, activation='relu', batch_norm=True,\n",
    "                             bias=True, weights_init='variance_scaling',\n",
    "                             bias_init='zeros', regularizer='L2', weight_decay=0.0001,\n",
    "                             trainable=True, restore=True, reuse=False, scope=None,\n",
    "                             name=\"ResidualBlock\", filter_size_residual=3):\n",
    "    # residual shrinkage blocks with channel-wise thresholds\n",
    "\n",
    "    residual = incoming\n",
    "    in_channels = incoming.get_shape().as_list()[-1]\n",
    "\n",
    "    # Variable Scope fix for older TF\n",
    "    try:\n",
    "        vscope = tf.variable_scope(scope, default_name=name, values=[incoming], reuse=reuse)\n",
    "    except Exception:\n",
    "        vscope = tf.variable_op_scope([incoming], scope, name, reuse=reuse)\n",
    "\n",
    "    with vscope as scope:\n",
    "        name = scope.name  # TODO\n",
    "\n",
    "        for i in range(nb_blocks):\n",
    "\n",
    "            identity = residual\n",
    "\n",
    "            if not downsample:\n",
    "                downsample_strides = 1\n",
    "\n",
    "            if batch_norm:\n",
    "                residual = tflearn.batch_normalization(residual)\n",
    "\n",
    "            residual = tflearn.activation(residual, activation)\n",
    "            residual = conv_2d(residual, out_channels, filter_size_residual,\n",
    "                               downsample_strides, 'same', 'linear',\n",
    "                               bias, weights_init, bias_init,\n",
    "                               regularizer, weight_decay, trainable,\n",
    "                               restore)\n",
    "\n",
    "            if batch_norm:\n",
    "                residual = tflearn.batch_normalization(residual)\n",
    "            residual = tflearn.activation(residual, activation)\n",
    "            residual = conv_2d(residual, out_channels, filter_size_residual, 1, 'same',\n",
    "                               'linear', bias, weights_init,\n",
    "                               bias_init, regularizer, weight_decay,\n",
    "                               trainable, restore)\n",
    "\n",
    "            # get thresholds and apply thresholding\n",
    "            abs_mean = tf.reduce_mean(tf.reduce_mean(tf.abs(residual), axis=2, keep_dims=True), axis=1, keep_dims=True)\n",
    "            scales = tflearn.fully_connected(abs_mean, n_units=out_channels // 4, activation='linear', regularizer='L2',\n",
    "                                             weight_decay=0.0001, weights_init='variance_scaling')\n",
    "            scales = tflearn.batch_normalization(scales)\n",
    "            scales = tflearn.activation(scales, 'relu')\n",
    "            scales = tflearn.fully_connected(scales, n_units=out_channels, activation='linear', regularizer='L2',\n",
    "                                             weight_decay=0.0001, weights_init='variance_scaling')\n",
    "            scales = tf.expand_dims(tf.expand_dims(scales, axis=1), axis=1)\n",
    "            thres = tf.multiply(abs_mean, tflearn.activations.sigmoid(scales))\n",
    "            # soft thresholding\n",
    "            residual = tf.multiply(tf.sign(residual), tf.maximum(tf.abs(residual) - thres, 0))\n",
    "\n",
    "            # Downsampling\n",
    "            if downsample_strides > 1:\n",
    "                identity = tflearn.avg_pool_2d(identity, 1,\n",
    "                                               downsample_strides)\n",
    "\n",
    "            # Projection to new dimension\n",
    "            if in_channels != out_channels:\n",
    "                if (out_channels - in_channels) % 2 == 0:\n",
    "                    ch = (out_channels - in_channels) // 2\n",
    "                    identity = tf.pad(identity,\n",
    "                                      [[0, 0], [0, 0], [0, 0], [ch, ch]])\n",
    "                else:\n",
    "                    ch = (out_channels - in_channels) // 2\n",
    "                    identity = tf.pad(identity,\n",
    "                                      [[0, 0], [0, 0], [0, 0], [ch, ch + 1]])\n",
    "                in_channels = out_channels\n",
    "\n",
    "            residual = residual + identity\n",
    "\n",
    "    return residual\n",
    "\n",
    "\n",
    "# Real-time data preprocessing\n",
    "img_prep = tflearn.ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center(per_channel=True)\n",
    "\n",
    "# Real-time data augmentation\n",
    "img_aug = tflearn.ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "\n",
    "# Build a Deep Residual Shrinkage Network with 3 blocks\n",
    "'''\n",
    "# Real-time data augmentation\n",
    "img_aug.add_random_crop([32, 32], padding=4)\n",
    "\n",
    "# Build a Deep Residual Shrinkage Network with 3 blocks\n",
    "net = tflearn.input_data(shape=[None, 32, 32, 3],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "net = tflearn.conv_2d(net, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "net = residual_shrinkage_block(net, 1, 16)\n",
    "net = residual_shrinkage_block(net, 1, 32, downsample=True)\n",
    "net = residual_shrinkage_block(net, 1, 32, downsample=True)\n",
    "\n",
    "nb_filter: `int`. The number of convolutional filters.\n",
    "filter_size: `int` or `list of int`. Size of filters.\n",
    "\n",
    "n_units: 神经元个数\n",
    "'''\n",
    "\n",
    "img_aug.add_random_crop([shape[0], shape[1]], padding=4)\n",
    "\n",
    "net = tflearn.input_data(shape=[None, shape[0], shape[1], shape[2]],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "net = tflearn.conv_2d(net, nb_filter=filter_number, filter_size=filter_size, regularizer='L2', weight_decay=0.0001)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number,\n",
    "                               filter_size_residual=filter_size)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number * 2,\n",
    "                               filter_size_residual=filter_size, downsample=True)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number * 2,\n",
    "                               filter_size_residual=filter_size, downsample=True)\n",
    "\n",
    "net = tflearn.batch_normalization(net)\n",
    "net = tflearn.activation(net, 'relu')\n",
    "net = tflearn.global_avg_pool(net)\n",
    "\n",
    "# Regression\n",
    "net = tflearn.fully_connected(net, n_units=classes, activation='softmax')\n",
    "mom = tflearn.Momentum(learning_rate=0.1, lr_decay=0.1, decay_step=20000, staircase=True)\n",
    "net = tflearn.regression(net, optimizer=mom, loss='categorical_crossentropy')\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(net, checkpoint_path='model-channel-coding', max_checkpoints=10,\n",
    "                    tensorboard_verbose=0, clip_gradients=0.)\n",
    "\n",
    "# epoch = 400\n",
    "\n",
    "model.fit(x_train, y_train, n_epoch=epoch, snapshot_epoch=False, snapshot_step=2000, show_metric=True,\n",
    "          batch_size=50, shuffle=True, run_id='model-channel-coding')\n",
    "\n",
    "\n",
    "def get_acc(epoch_read):\n",
    "    training_acc = model.evaluate(x_train, y_train)\n",
    "    validation_acc = model.evaluate(x_valid, y_valid)\n",
    "    print(\"\\n\")\n",
    "    print(\"tranining_acc = {}\".format(training_acc))\n",
    "    print(\"validation_acc = {}\".format(validation_acc))\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    for i in range(len(y_pred)):\n",
    "        max_value = max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value == y_pred[i][j]:\n",
    "                y_pred[i][j] = 1\n",
    "            else:\n",
    "                y_pred[i][j] = 0\n",
    "\n",
    "    y_pred_num = []\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_valid)):\n",
    "            if all(y_pred[i] == y_valid[j]):\n",
    "                y_pred_num.append(y_valid_num[j])\n",
    "                break\n",
    "\n",
    "    y_pred_class = encoder.inverse_transform(y_pred_num)\n",
    "    validation_report = classification_report(y_valid_class, y_pred_class, digits=8)\n",
    "    print(validation_report)\n",
    "\n",
    "    with open(\"{}.txt\".format(datetime.datetime.now().strftime('%Y-%m-%d %H-%M-%S.%f')), \"w\") as f:\n",
    "        f.write(\"filter_number = {}\\n\".format(filter_number))\n",
    "        f.write(\"filter_size = {}\\n\".format(filter_size))\n",
    "        f.write(\"file_name = {}\\n\".format(file_name))\n",
    "        f.write(\"modulate = {}\\n\".format('bpsk'))\n",
    "        f.write(\"epoch = {}\\n\".format(epoch_read))\n",
    "        if is_turn:\n",
    "            f.write(\"turn = {}\\n\".format(turn))\n",
    "        f.write(\"training_acc = {}\\n\".format(training_acc))\n",
    "        f.write(\"validation_acc = {}\\n\".format(validation_acc))\n",
    "        f.write(validation_report)\n",
    "\n",
    "\n",
    "get_acc(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset-awgn-conv-0db-pre-raw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-62d6a9d119bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset-awgn-conv-0db-pre-raw.csv'"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import pandas\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tflearn\n",
    "import numpy\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from tflearn.layers.conv import conv_2d\n",
    "\n",
    "# Data loading\n",
    "from tflearn.datasets import cifar10\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# filter_number = int(sys.argv[1])\n",
    "# filter_size = int(sys.argv[2])\n",
    "\n",
    "\n",
    "filter_number = 16\n",
    "filter_size = 4\n",
    "\n",
    "is_turn = 0\n",
    "turn = 0\n",
    "\n",
    "# snr = sys.argv[1]\n",
    "# epoch = int(sys.argv[2])\n",
    "# turn = sys.argv[2]\n",
    "# is_turn = 1\n",
    "# file_name = \"dataset-awgn-conv-\" + str(snr) + \"db-pre-raw.csv\"\n",
    "file_name = \"dataset-awgn-conv-0db-pre-raw.csv\"\n",
    "\n",
    "classes = 15\n",
    "shape = [32, 32, 4]\n",
    "pretreatment = 3\n",
    "\n",
    "\n",
    "def get_dataset(matrix):\n",
    "    dataset = []\n",
    "    for s in matrix:\n",
    "        a = s.strip().split(' ')\n",
    "        col = []\n",
    "        for b in a:\n",
    "            row = []\n",
    "            b = float(b)\n",
    "            row.append(b)\n",
    "            col.append(row)\n",
    "        if pretreatment == 1:\n",
    "            # gfft\n",
    "            col.append([0, 0])\n",
    "        elif pretreatment == 2:\n",
    "            # llr cal\n",
    "            col.append([0])\n",
    "        elif pretreatment == 3:\n",
    "            # llr raw\n",
    "            i = 96\n",
    "            for j in range(i):\n",
    "                col.append([0])\n",
    "        dataset.append(numpy.array([col]).reshape((shape[0], shape[1], shape[2])))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataframe = pandas.read_csv(file_name)\n",
    "X = numpy.array(get_dataset(dataframe['X']))\n",
    "Y = dataframe['Y']\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(X, Y)\n",
    "x_train = X\n",
    "y_train = Y\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_valid_class = y_valid\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)\n",
    "y_valid_num = y_valid\n",
    "\n",
    "# X = numpy.expand_dims(X, axis=0)\n",
    "# X = numpy.expand_dims(X, axis=0)\n",
    "# testX = numpy.expand_dims(testX, axis=0)\n",
    "# testX = numpy.expand_dims(testX, axis=\n",
    "\n",
    "# Add noise\n",
    "# X = X + numpy.random.random((50000, 32, 32, 3)) * 0.1\n",
    "# testX = testX + numpy.random.random((10000, 32, 32, 3)) * 0.1\n",
    "\n",
    "# Transform labels to one-hot format\n",
    "# Y = tflearn.data_utils.to_categorical(Y, 10)\n",
    "# testY = tflearn.data_utils.to_categorical(testY, 10)\n",
    "y_train = tflearn.data_utils.to_categorical(y_train, classes)\n",
    "y_valid = tflearn.data_utils.to_categorical(y_valid, classes)\n",
    "\n",
    "\n",
    "def residual_shrinkage_block(incoming, nb_blocks, out_channels, downsample=False,\n",
    "                             downsample_strides=2, activation='relu', batch_norm=True,\n",
    "                             bias=True, weights_init='variance_scaling',\n",
    "                             bias_init='zeros', regularizer='L2', weight_decay=0.0001,\n",
    "                             trainable=True, restore=True, reuse=False, scope=None,\n",
    "                             name=\"ResidualBlock\", filter_size_residual=3):\n",
    "    # residual shrinkage blocks with channel-wise thresholds\n",
    "\n",
    "    residual = incoming\n",
    "    in_channels = incoming.get_shape().as_list()[-1]\n",
    "\n",
    "    # Variable Scope fix for older TF\n",
    "    try:\n",
    "        vscope = tf.variable_scope(scope, default_name=name, values=[incoming], reuse=reuse)\n",
    "    except Exception:\n",
    "        vscope = tf.variable_op_scope([incoming], scope, name, reuse=reuse)\n",
    "\n",
    "    with vscope as scope:\n",
    "        name = scope.name  # TODO\n",
    "\n",
    "        for i in range(nb_blocks):\n",
    "\n",
    "            identity = residual\n",
    "\n",
    "            if not downsample:\n",
    "                downsample_strides = 1\n",
    "\n",
    "            if batch_norm:\n",
    "                residual = tflearn.batch_normalization(residual)\n",
    "\n",
    "            residual = tflearn.activation(residual, activation)\n",
    "            residual = conv_2d(residual, out_channels, filter_size_residual,\n",
    "                               downsample_strides, 'same', 'linear',\n",
    "                               bias, weights_init, bias_init,\n",
    "                               regularizer, weight_decay, trainable,\n",
    "                               restore)\n",
    "\n",
    "            if batch_norm:\n",
    "                residual = tflearn.batch_normalization(residual)\n",
    "            residual = tflearn.activation(residual, activation)\n",
    "            residual = conv_2d(residual, out_channels, filter_size_residual, 1, 'same',\n",
    "                               'linear', bias, weights_init,\n",
    "                               bias_init, regularizer, weight_decay,\n",
    "                               trainable, restore)\n",
    "\n",
    "            # get thresholds and apply thresholding\n",
    "            abs_mean = tf.reduce_mean(tf.reduce_mean(tf.abs(residual), axis=2, keep_dims=True), axis=1, keep_dims=True)\n",
    "            scales = tflearn.fully_connected(abs_mean, n_units=out_channels // 4, activation='linear', regularizer='L2',\n",
    "                                             weight_decay=0.0001, weights_init='variance_scaling')\n",
    "            scales = tflearn.batch_normalization(scales)\n",
    "            scales = tflearn.activation(scales, 'relu')\n",
    "            scales = tflearn.fully_connected(scales, n_units=out_channels, activation='linear', regularizer='L2',\n",
    "                                             weight_decay=0.0001, weights_init='variance_scaling')\n",
    "            scales = tf.expand_dims(tf.expand_dims(scales, axis=1), axis=1)\n",
    "            thres = tf.multiply(abs_mean, tflearn.activations.sigmoid(scales))\n",
    "            # soft thresholding\n",
    "            residual = tf.multiply(tf.sign(residual), tf.maximum(tf.abs(residual) - thres, 0))\n",
    "\n",
    "            # Downsampling\n",
    "            if downsample_strides > 1:\n",
    "                identity = tflearn.avg_pool_2d(identity, 1,\n",
    "                                               downsample_strides)\n",
    "\n",
    "            # Projection to new dimension\n",
    "            if in_channels != out_channels:\n",
    "                if (out_channels - in_channels) % 2 == 0:\n",
    "                    ch = (out_channels - in_channels) // 2\n",
    "                    identity = tf.pad(identity,\n",
    "                                      [[0, 0], [0, 0], [0, 0], [ch, ch]])\n",
    "                else:\n",
    "                    ch = (out_channels - in_channels) // 2\n",
    "                    identity = tf.pad(identity,\n",
    "                                      [[0, 0], [0, 0], [0, 0], [ch, ch + 1]])\n",
    "                in_channels = out_channels\n",
    "\n",
    "            residual = residual + identity\n",
    "\n",
    "    return residual\n",
    "\n",
    "\n",
    "# Real-time data preprocessing\n",
    "img_prep = tflearn.ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center(per_channel=True)\n",
    "\n",
    "# Real-time data augmentation\n",
    "img_aug = tflearn.ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "\n",
    "# Build a Deep Residual Shrinkage Network with 3 blocks\n",
    "'''\n",
    "# Real-time data augmentation\n",
    "img_aug.add_random_crop([32, 32], padding=4)\n",
    "\n",
    "# Build a Deep Residual Shrinkage Network with 3 blocks\n",
    "net = tflearn.input_data(shape=[None, 32, 32, 3],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "net = tflearn.conv_2d(net, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "net = residual_shrinkage_block(net, 1, 16)\n",
    "net = residual_shrinkage_block(net, 1, 32, downsample=True)\n",
    "net = residual_shrinkage_block(net, 1, 32, downsample=True)\n",
    "\n",
    "nb_filter: `int`. The number of convolutional filters.\n",
    "filter_size: `int` or `list of int`. Size of filters.\n",
    "\n",
    "n_units: 神经元个数\n",
    "'''\n",
    "\n",
    "img_aug.add_random_crop([shape[0], shape[1]], padding=4)\n",
    "\n",
    "net = tflearn.input_data(shape=[None, shape[0], shape[1], shape[2]],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "net = tflearn.conv_2d(net, nb_filter=filter_number, filter_size=filter_size, regularizer='L2', weight_decay=0.0001)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number,\n",
    "                               filter_size_residual=filter_size)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number * 2,\n",
    "                               filter_size_residual=filter_size, downsample=True)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number * 2,\n",
    "                               filter_size_residual=filter_size, downsample=True)\n",
    "\n",
    "net = tflearn.batch_normalization(net)\n",
    "net = tflearn.activation(net, 'relu')\n",
    "net = tflearn.global_avg_pool(net)\n",
    "\n",
    "# Regression\n",
    "net = tflearn.fully_connected(net, n_units=classes, activation='softmax')\n",
    "mom = tflearn.Momentum(learning_rate=0.1, lr_decay=0.1, decay_step=20000, staircase=True)\n",
    "net = tflearn.regression(net, optimizer=mom, loss='categorical_crossentropy')\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(net, checkpoint_path='model-channel-coding', max_checkpoints=10,\n",
    "                    tensorboard_verbose=0, clip_gradients=0.)\n",
    "\n",
    "# epoch = 400\n",
    "\n",
    "model.fit(x_train, y_train, n_epoch=epoch, snapshot_epoch=False, snapshot_step=2000, show_metric=True,\n",
    "          batch_size=50, shuffle=True, run_id='model-channel-coding')\n",
    "\n",
    "\n",
    "def get_acc(epoch_read):\n",
    "    training_acc = model.evaluate(x_train, y_train)\n",
    "    validation_acc = model.evaluate(x_valid, y_valid)\n",
    "    print(\"\\n\")\n",
    "    print(\"tranining_acc = {}\".format(training_acc))\n",
    "    print(\"validation_acc = {}\".format(validation_acc))\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    for i in range(len(y_pred)):\n",
    "        max_value = max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value == y_pred[i][j]:\n",
    "                y_pred[i][j] = 1\n",
    "            else:\n",
    "                y_pred[i][j] = 0\n",
    "\n",
    "    y_pred_num = []\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_valid)):\n",
    "            if all(y_pred[i] == y_valid[j]):\n",
    "                y_pred_num.append(y_valid_num[j])\n",
    "                break\n",
    "\n",
    "    y_pred_class = encoder.inverse_transform(y_pred_num)\n",
    "    validation_report = classification_report(y_valid_class, y_pred_class, digits=8)\n",
    "    print(validation_report)\n",
    "\n",
    "    with open(\"{}.txt\".format(datetime.datetime.now().strftime('%Y-%m-%d %H-%M-%S.%f')), \"w\") as f:\n",
    "        f.write(\"filter_number = {}\\n\".format(filter_number))\n",
    "        f.write(\"filter_size = {}\\n\".format(filter_size))\n",
    "        f.write(\"file_name = {}\\n\".format(file_name))\n",
    "        f.write(\"modulate = {}\\n\".format('bpsk'))\n",
    "        f.write(\"epoch = {}\\n\".format(epoch_read))\n",
    "        if is_turn:\n",
    "            f.write(\"turn = {}\\n\".format(turn))\n",
    "        f.write(\"training_acc = {}\\n\".format(training_acc))\n",
    "        f.write(\"validation_acc = {}\\n\".format(validation_acc))\n",
    "        f.write(validation_report)\n",
    "\n",
    "\n",
    "get_acc(epoch)\n",
    "\n",
    "# n = int(epoch / 100)\n",
    "# for e in range(n):\n",
    "#     epoch_read = (e + 1) * 100\n",
    "#     model.load(\"model-channel-coding-{}\".format(epoch_read * 20))\n",
    "#     get_acc(model, epoch_read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset-awgn-conv-0db-pre-raw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1861ad284795>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\soft\\anaconda\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset-awgn-conv-0db-pre-raw.csv'"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import pandas\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tflearn\n",
    "import numpy\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from tflearn.layers.conv import conv_2d\n",
    "\n",
    "# Data loading\n",
    "from tflearn.datasets import cifar10\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# filter_number = int(sys.argv[1])\n",
    "# filter_size = int(sys.argv[2])\n",
    "\n",
    "\n",
    "filter_number = 16\n",
    "filter_size = 4\n",
    "\n",
    "is_turn = 0\n",
    "turn = 0\n",
    "\n",
    "# snr = sys.argv[1]\n",
    "# epoch = int(sys.argv[2])\n",
    "epoch = 400\n",
    "# turn = sys.argv[2]\n",
    "# is_turn = 1\n",
    "# file_name = \"dataset-awgn-conv-\" + str(snr) + \"db-pre-raw.csv\"\n",
    "file_name = \"dataset-awgn-conv-0db-pre-raw.csv\"\n",
    "\n",
    "classes = 15\n",
    "shape = [32, 32, 4]\n",
    "pretreatment = 3\n",
    "\n",
    "\n",
    "def get_dataset(matrix):\n",
    "    dataset = []\n",
    "    for s in matrix:\n",
    "        a = s.strip().split(' ')\n",
    "        col = []\n",
    "        for b in a:\n",
    "            row = []\n",
    "            b = float(b)\n",
    "            row.append(b)\n",
    "            col.append(row)\n",
    "        if pretreatment == 1:\n",
    "            # gfft\n",
    "            col.append([0, 0])\n",
    "        elif pretreatment == 2:\n",
    "            # llr cal\n",
    "            col.append([0])\n",
    "        elif pretreatment == 3:\n",
    "            # llr raw\n",
    "            i = 96\n",
    "            for j in range(i):\n",
    "                col.append([0])\n",
    "        dataset.append(numpy.array([col]).reshape((shape[0], shape[1], shape[2])))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataframe = pandas.read_csv(file_name)\n",
    "X = numpy.array(get_dataset(dataframe['X']))\n",
    "Y = dataframe['Y']\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(X, Y)\n",
    "x_train = X\n",
    "y_train = Y\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_valid_class = y_valid\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)\n",
    "y_valid_num = y_valid\n",
    "\n",
    "# X = numpy.expand_dims(X, axis=0)\n",
    "# X = numpy.expand_dims(X, axis=0)\n",
    "# testX = numpy.expand_dims(testX, axis=0)\n",
    "# testX = numpy.expand_dims(testX, axis=\n",
    "\n",
    "# Add noise\n",
    "# X = X + numpy.random.random((50000, 32, 32, 3)) * 0.1\n",
    "# testX = testX + numpy.random.random((10000, 32, 32, 3)) * 0.1\n",
    "\n",
    "# Transform labels to one-hot format\n",
    "# Y = tflearn.data_utils.to_categorical(Y, 10)\n",
    "# testY = tflearn.data_utils.to_categorical(testY, 10)\n",
    "y_train = tflearn.data_utils.to_categorical(y_train, classes)\n",
    "y_valid = tflearn.data_utils.to_categorical(y_valid, classes)\n",
    "\n",
    "\n",
    "def residual_shrinkage_block(incoming, nb_blocks, out_channels, downsample=False,\n",
    "                             downsample_strides=2, activation='relu', batch_norm=True,\n",
    "                             bias=True, weights_init='variance_scaling',\n",
    "                             bias_init='zeros', regularizer='L2', weight_decay=0.0001,\n",
    "                             trainable=True, restore=True, reuse=False, scope=None,\n",
    "                             name=\"ResidualBlock\", filter_size_residual=3):\n",
    "    # residual shrinkage blocks with channel-wise thresholds\n",
    "\n",
    "    residual = incoming\n",
    "    in_channels = incoming.get_shape().as_list()[-1]\n",
    "\n",
    "    # Variable Scope fix for older TF\n",
    "    try:\n",
    "        vscope = tf.variable_scope(scope, default_name=name, values=[incoming], reuse=reuse)\n",
    "    except Exception:\n",
    "        vscope = tf.variable_op_scope([incoming], scope, name, reuse=reuse)\n",
    "\n",
    "    with vscope as scope:\n",
    "        name = scope.name  # TODO\n",
    "\n",
    "        for i in range(nb_blocks):\n",
    "\n",
    "            identity = residual\n",
    "\n",
    "            if not downsample:\n",
    "                downsample_strides = 1\n",
    "\n",
    "            if batch_norm:\n",
    "                residual = tflearn.batch_normalization(residual)\n",
    "\n",
    "            residual = tflearn.activation(residual, activation)\n",
    "            residual = conv_2d(residual, out_channels, filter_size_residual,\n",
    "                               downsample_strides, 'same', 'linear',\n",
    "                               bias, weights_init, bias_init,\n",
    "                               regularizer, weight_decay, trainable,\n",
    "                               restore)\n",
    "\n",
    "            if batch_norm:\n",
    "                residual = tflearn.batch_normalization(residual)\n",
    "            residual = tflearn.activation(residual, activation)\n",
    "            residual = conv_2d(residual, out_channels, filter_size_residual, 1, 'same',\n",
    "                               'linear', bias, weights_init,\n",
    "                               bias_init, regularizer, weight_decay,\n",
    "                               trainable, restore)\n",
    "\n",
    "            # get thresholds and apply thresholding\n",
    "            abs_mean = tf.reduce_mean(tf.reduce_mean(tf.abs(residual), axis=2, keep_dims=True), axis=1, keep_dims=True)\n",
    "            scales = tflearn.fully_connected(abs_mean, n_units=out_channels // 4, activation='linear', regularizer='L2',\n",
    "                                             weight_decay=0.0001, weights_init='variance_scaling')\n",
    "            scales = tflearn.batch_normalization(scales)\n",
    "            scales = tflearn.activation(scales, 'relu')\n",
    "            scales = tflearn.fully_connected(scales, n_units=out_channels, activation='linear', regularizer='L2',\n",
    "                                             weight_decay=0.0001, weights_init='variance_scaling')\n",
    "            scales = tf.expand_dims(tf.expand_dims(scales, axis=1), axis=1)\n",
    "            thres = tf.multiply(abs_mean, tflearn.activations.sigmoid(scales))\n",
    "            # soft thresholding\n",
    "            residual = tf.multiply(tf.sign(residual), tf.maximum(tf.abs(residual) - thres, 0))\n",
    "\n",
    "            # Downsampling\n",
    "            if downsample_strides > 1:\n",
    "                identity = tflearn.avg_pool_2d(identity, 1,\n",
    "                                               downsample_strides)\n",
    "\n",
    "            # Projection to new dimension\n",
    "            if in_channels != out_channels:\n",
    "                if (out_channels - in_channels) % 2 == 0:\n",
    "                    ch = (out_channels - in_channels) // 2\n",
    "                    identity = tf.pad(identity,\n",
    "                                      [[0, 0], [0, 0], [0, 0], [ch, ch]])\n",
    "                else:\n",
    "                    ch = (out_channels - in_channels) // 2\n",
    "                    identity = tf.pad(identity,\n",
    "                                      [[0, 0], [0, 0], [0, 0], [ch, ch + 1]])\n",
    "                in_channels = out_channels\n",
    "\n",
    "            residual = residual + identity\n",
    "\n",
    "    return residual\n",
    "\n",
    "\n",
    "# Real-time data preprocessing\n",
    "img_prep = tflearn.ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center(per_channel=True)\n",
    "\n",
    "# Real-time data augmentation\n",
    "img_aug = tflearn.ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "\n",
    "# Build a Deep Residual Shrinkage Network with 3 blocks\n",
    "'''\n",
    "# Real-time data augmentation\n",
    "img_aug.add_random_crop([32, 32], padding=4)\n",
    "\n",
    "# Build a Deep Residual Shrinkage Network with 3 blocks\n",
    "net = tflearn.input_data(shape=[None, 32, 32, 3],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "net = tflearn.conv_2d(net, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "net = residual_shrinkage_block(net, 1, 16)\n",
    "net = residual_shrinkage_block(net, 1, 32, downsample=True)\n",
    "net = residual_shrinkage_block(net, 1, 32, downsample=True)\n",
    "\n",
    "nb_filter: `int`. The number of convolutional filters.\n",
    "filter_size: `int` or `list of int`. Size of filters.\n",
    "\n",
    "n_units: 神经元个数\n",
    "'''\n",
    "\n",
    "img_aug.add_random_crop([shape[0], shape[1]], padding=4)\n",
    "\n",
    "net = tflearn.input_data(shape=[None, shape[0], shape[1], shape[2]],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "net = tflearn.conv_2d(net, nb_filter=filter_number, filter_size=filter_size, regularizer='L2', weight_decay=0.0001)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number,\n",
    "                               filter_size_residual=filter_size)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number * 2,\n",
    "                               filter_size_residual=filter_size, downsample=True)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number * 2,\n",
    "                               filter_size_residual=filter_size, downsample=True)\n",
    "\n",
    "net = tflearn.batch_normalization(net)\n",
    "net = tflearn.activation(net, 'relu')\n",
    "net = tflearn.global_avg_pool(net)\n",
    "\n",
    "# Regression\n",
    "net = tflearn.fully_connected(net, n_units=classes, activation='softmax')\n",
    "mom = tflearn.Momentum(learning_rate=0.1, lr_decay=0.1, decay_step=20000, staircase=True)\n",
    "net = tflearn.regression(net, optimizer=mom, loss='categorical_crossentropy')\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(net, checkpoint_path='model-channel-coding', max_checkpoints=10,\n",
    "                    tensorboard_verbose=0, clip_gradients=0.)\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, n_epoch=epoch, snapshot_epoch=False, snapshot_step=2000, show_metric=True,\n",
    "          batch_size=50, shuffle=True, run_id='model-channel-coding')\n",
    "\n",
    "\n",
    "def get_acc(epoch_read):\n",
    "    training_acc = model.evaluate(x_train, y_train)\n",
    "    validation_acc = model.evaluate(x_valid, y_valid)\n",
    "    print(\"\\n\")\n",
    "    print(\"tranining_acc = {}\".format(training_acc))\n",
    "    print(\"validation_acc = {}\".format(validation_acc))\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    for i in range(len(y_pred)):\n",
    "        max_value = max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value == y_pred[i][j]:\n",
    "                y_pred[i][j] = 1\n",
    "            else:\n",
    "                y_pred[i][j] = 0\n",
    "\n",
    "    y_pred_num = []\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_valid)):\n",
    "            if all(y_pred[i] == y_valid[j]):\n",
    "                y_pred_num.append(y_valid_num[j])\n",
    "                break\n",
    "\n",
    "    y_pred_class = encoder.inverse_transform(y_pred_num)\n",
    "    validation_report = classification_report(y_valid_class, y_pred_class, digits=8)\n",
    "    print(validation_report)\n",
    "\n",
    "    with open(\"{}.txt\".format(datetime.datetime.now().strftime('%Y-%m-%d %H-%M-%S.%f')), \"w\") as f:\n",
    "        f.write(\"filter_number = {}\\n\".format(filter_number))\n",
    "        f.write(\"filter_size = {}\\n\".format(filter_size))\n",
    "        f.write(\"file_name = {}\\n\".format(file_name))\n",
    "        f.write(\"modulate = {}\\n\".format('bpsk'))\n",
    "        f.write(\"epoch = {}\\n\".format(epoch_read))\n",
    "        if is_turn:\n",
    "            f.write(\"turn = {}\\n\".format(turn))\n",
    "        f.write(\"training_acc = {}\\n\".format(training_acc))\n",
    "        f.write(\"validation_acc = {}\\n\".format(validation_acc))\n",
    "        f.write(validation_report)\n",
    "\n",
    "\n",
    "get_acc(epoch)\n",
    "\n",
    "# n = int(epoch / 100)\n",
    "# for e in range(n):\n",
    "#     epoch_read = (e + 1) * 100\n",
    "#     model.load(\"model-channel-coding-{}\".format(epoch_read * 20))\n",
    "#     get_acc(model, epoch_read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7999  | total loss: \u001b[1m\u001b[32m0.29925\u001b[0m\u001b[0m | time: 7.867s\n",
      "| Momentum | epoch: 400 | loss: 0.29925 - acc: 0.9287 -- iter: 0950/1000\n",
      "Training Step: 8000  | total loss: \u001b[1m\u001b[32m0.29094\u001b[0m\u001b[0m | time: 8.262s\n",
      "| Momentum | epoch: 400 | loss: 0.29094 - acc: 0.9298 -- iter: 1000/1000\n",
      "--\n",
      "\n",
      "\n",
      "tranining_acc = [0.46200000095367433]\n",
      "validation_acc = [0.47599999952316285]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " conv（2，1，3）  1.00000000 0.55072464 0.71028037        69\n",
      " conv（2，1，4）  0.50000000 0.01492537 0.02898551        67\n",
      " conv（2，1，5）  0.32642487 1.00000000 0.49218750        63\n",
      " conv（2，1，6）  1.00000000 0.33333333 0.50000000        51\n",
      "\n",
      "    accuracy                      0.47600000       250\n",
      "   macro avg  0.70660622 0.47474584 0.43286335       250\n",
      "weighted avg  0.69625907 0.47600000 0.42983675       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import pandas\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tflearn\n",
    "import numpy\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from tflearn.layers.conv import conv_2d\n",
    "\n",
    "# Data loading\n",
    "from tflearn.datasets import cifar10\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# filter_number = int(sys.argv[1])\n",
    "# filter_size = int(sys.argv[2])\n",
    "\n",
    "\n",
    "filter_number = 16\n",
    "filter_size = 4\n",
    "\n",
    "is_turn = 0\n",
    "turn = 0\n",
    "\n",
    "# snr = sys.argv[1]\n",
    "# epoch = int(sys.argv[2])\n",
    "epoch = 400\n",
    "# turn = sys.argv[2]\n",
    "# is_turn = 1\n",
    "# file_name = \"dataset-awgn-conv-\" + str(snr) + \"db-pre-raw.csv\"\n",
    "file_name = \"dataset-awgn-conv-0db-pre-raw.csv\"\n",
    "\n",
    "classes = 15\n",
    "shape = [32, 32, 4]\n",
    "pretreatment = 3\n",
    "\n",
    "\n",
    "def get_dataset(matrix):\n",
    "    dataset = []\n",
    "    for s in matrix:\n",
    "        a = s.strip().split(' ')\n",
    "        col = []\n",
    "        for b in a:\n",
    "            row = []\n",
    "            b = float(b)\n",
    "            row.append(b)\n",
    "            col.append(row)\n",
    "        if pretreatment == 1:\n",
    "            # gfft\n",
    "            col.append([0, 0])\n",
    "        elif pretreatment == 2:\n",
    "            # llr cal\n",
    "            col.append([0])\n",
    "        elif pretreatment == 3:\n",
    "            # llr raw\n",
    "            i = 96\n",
    "            for j in range(i):\n",
    "                col.append([0])\n",
    "        dataset.append(numpy.array([col]).reshape((shape[0], shape[1], shape[2])))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataframe = pandas.read_csv(file_name)\n",
    "X = numpy.array(get_dataset(dataframe['X']))\n",
    "Y = dataframe['Y']\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(X, Y)\n",
    "x_train = X\n",
    "y_train = Y\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_valid_class = y_valid\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)\n",
    "y_valid_num = y_valid\n",
    "\n",
    "# X = numpy.expand_dims(X, axis=0)\n",
    "# X = numpy.expand_dims(X, axis=0)\n",
    "# testX = numpy.expand_dims(testX, axis=0)\n",
    "# testX = numpy.expand_dims(testX, axis=\n",
    "\n",
    "# Add noise\n",
    "# X = X + numpy.random.random((50000, 32, 32, 3)) * 0.1\n",
    "# testX = testX + numpy.random.random((10000, 32, 32, 3)) * 0.1\n",
    "\n",
    "# Transform labels to one-hot format\n",
    "# Y = tflearn.data_utils.to_categorical(Y, 10)\n",
    "# testY = tflearn.data_utils.to_categorical(testY, 10)\n",
    "y_train = tflearn.data_utils.to_categorical(y_train, classes)\n",
    "y_valid = tflearn.data_utils.to_categorical(y_valid, classes)\n",
    "\n",
    "\n",
    "def residual_shrinkage_block(incoming, nb_blocks, out_channels, downsample=False,\n",
    "                             downsample_strides=2, activation='relu', batch_norm=True,\n",
    "                             bias=True, weights_init='variance_scaling',\n",
    "                             bias_init='zeros', regularizer='L2', weight_decay=0.0001,\n",
    "                             trainable=True, restore=True, reuse=False, scope=None,\n",
    "                             name=\"ResidualBlock\", filter_size_residual=3):\n",
    "    # residual shrinkage blocks with channel-wise thresholds\n",
    "\n",
    "    residual = incoming\n",
    "    in_channels = incoming.get_shape().as_list()[-1]\n",
    "\n",
    "    # Variable Scope fix for older TF\n",
    "    try:\n",
    "        vscope = tf.variable_scope(scope, default_name=name, values=[incoming], reuse=reuse)\n",
    "    except Exception:\n",
    "        vscope = tf.variable_op_scope([incoming], scope, name, reuse=reuse)\n",
    "\n",
    "    with vscope as scope:\n",
    "        name = scope.name  # TODO\n",
    "\n",
    "        for i in range(nb_blocks):\n",
    "\n",
    "            identity = residual\n",
    "\n",
    "            if not downsample:\n",
    "                downsample_strides = 1\n",
    "\n",
    "            if batch_norm:\n",
    "                residual = tflearn.batch_normalization(residual)\n",
    "\n",
    "            residual = tflearn.activation(residual, activation)\n",
    "            residual = conv_2d(residual, out_channels, filter_size_residual,\n",
    "                               downsample_strides, 'same', 'linear',\n",
    "                               bias, weights_init, bias_init,\n",
    "                               regularizer, weight_decay, trainable,\n",
    "                               restore)\n",
    "\n",
    "            if batch_norm:\n",
    "                residual = tflearn.batch_normalization(residual)\n",
    "            residual = tflearn.activation(residual, activation)\n",
    "            residual = conv_2d(residual, out_channels, filter_size_residual, 1, 'same',\n",
    "                               'linear', bias, weights_init,\n",
    "                               bias_init, regularizer, weight_decay,\n",
    "                               trainable, restore)\n",
    "\n",
    "            # get thresholds and apply thresholding\n",
    "            abs_mean = tf.reduce_mean(tf.reduce_mean(tf.abs(residual), axis=2, keep_dims=True), axis=1, keep_dims=True)\n",
    "            scales = tflearn.fully_connected(abs_mean, n_units=out_channels // 4, activation='linear', regularizer='L2',\n",
    "                                             weight_decay=0.0001, weights_init='variance_scaling')\n",
    "            scales = tflearn.batch_normalization(scales)\n",
    "            scales = tflearn.activation(scales, 'relu')\n",
    "            scales = tflearn.fully_connected(scales, n_units=out_channels, activation='linear', regularizer='L2',\n",
    "                                             weight_decay=0.0001, weights_init='variance_scaling')\n",
    "            scales = tf.expand_dims(tf.expand_dims(scales, axis=1), axis=1)\n",
    "            thres = tf.multiply(abs_mean, tflearn.activations.sigmoid(scales))\n",
    "            # soft thresholding\n",
    "            residual = tf.multiply(tf.sign(residual), tf.maximum(tf.abs(residual) - thres, 0))\n",
    "\n",
    "            # Downsampling\n",
    "            if downsample_strides > 1:\n",
    "                identity = tflearn.avg_pool_2d(identity, 1,\n",
    "                                               downsample_strides)\n",
    "\n",
    "            # Projection to new dimension\n",
    "            if in_channels != out_channels:\n",
    "                if (out_channels - in_channels) % 2 == 0:\n",
    "                    ch = (out_channels - in_channels) // 2\n",
    "                    identity = tf.pad(identity,\n",
    "                                      [[0, 0], [0, 0], [0, 0], [ch, ch]])\n",
    "                else:\n",
    "                    ch = (out_channels - in_channels) // 2\n",
    "                    identity = tf.pad(identity,\n",
    "                                      [[0, 0], [0, 0], [0, 0], [ch, ch + 1]])\n",
    "                in_channels = out_channels\n",
    "\n",
    "            residual = residual + identity\n",
    "\n",
    "    return residual\n",
    "\n",
    "\n",
    "# Real-time data preprocessing\n",
    "img_prep = tflearn.ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center(per_channel=True)\n",
    "\n",
    "# Real-time data augmentation\n",
    "img_aug = tflearn.ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "\n",
    "# Build a Deep Residual Shrinkage Network with 3 blocks\n",
    "'''\n",
    "# Real-time data augmentation\n",
    "img_aug.add_random_crop([32, 32], padding=4)\n",
    "\n",
    "# Build a Deep Residual Shrinkage Network with 3 blocks\n",
    "net = tflearn.input_data(shape=[None, 32, 32, 3],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "net = tflearn.conv_2d(net, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "net = residual_shrinkage_block(net, 1, 16)\n",
    "net = residual_shrinkage_block(net, 1, 32, downsample=True)\n",
    "net = residual_shrinkage_block(net, 1, 32, downsample=True)\n",
    "\n",
    "nb_filter: `int`. The number of convolutional filters.\n",
    "filter_size: `int` or `list of int`. Size of filters.\n",
    "\n",
    "n_units: 神经元个数\n",
    "'''\n",
    "\n",
    "img_aug.add_random_crop([shape[0], shape[1]], padding=4)\n",
    "\n",
    "net = tflearn.input_data(shape=[None, shape[0], shape[1], shape[2]],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "net = tflearn.conv_2d(net, nb_filter=filter_number, filter_size=filter_size, regularizer='L2', weight_decay=0.0001)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number,\n",
    "                               filter_size_residual=filter_size)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number * 2,\n",
    "                               filter_size_residual=filter_size, downsample=True)\n",
    "net = residual_shrinkage_block(net, nb_blocks=1, out_channels=filter_number * 2,\n",
    "                               filter_size_residual=filter_size, downsample=True)\n",
    "\n",
    "net = tflearn.batch_normalization(net)\n",
    "net = tflearn.activation(net, 'relu')\n",
    "net = tflearn.global_avg_pool(net)\n",
    "\n",
    "# Regression\n",
    "net = tflearn.fully_connected(net, n_units=classes, activation='softmax')\n",
    "mom = tflearn.Momentum(learning_rate=0.1, lr_decay=0.1, decay_step=20000, staircase=True)\n",
    "net = tflearn.regression(net, optimizer=mom, loss='categorical_crossentropy')\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(net, checkpoint_path='model-channel-coding', max_checkpoints=10,\n",
    "                    tensorboard_verbose=0, clip_gradients=0.)\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, n_epoch=epoch, snapshot_epoch=False, snapshot_step=2000, show_metric=True,\n",
    "          batch_size=50, shuffle=True, run_id='model-channel-coding')\n",
    "\n",
    "\n",
    "def get_acc(epoch_read):\n",
    "    training_acc = model.evaluate(x_train, y_train)\n",
    "    validation_acc = model.evaluate(x_valid, y_valid)\n",
    "    print(\"\\n\")\n",
    "    print(\"tranining_acc = {}\".format(training_acc))\n",
    "    print(\"validation_acc = {}\".format(validation_acc))\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    for i in range(len(y_pred)):\n",
    "        max_value = max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value == y_pred[i][j]:\n",
    "                y_pred[i][j] = 1\n",
    "            else:\n",
    "                y_pred[i][j] = 0\n",
    "\n",
    "    y_pred_num = []\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_valid)):\n",
    "            if all(y_pred[i] == y_valid[j]):\n",
    "                y_pred_num.append(y_valid_num[j])\n",
    "                break\n",
    "\n",
    "    y_pred_class = encoder.inverse_transform(y_pred_num)\n",
    "    validation_report = classification_report(y_valid_class, y_pred_class, digits=8)\n",
    "    print(validation_report)\n",
    "\n",
    "    with open(\"{}.txt\".format(datetime.datetime.now().strftime('%Y-%m-%d %H-%M-%S.%f')), \"w\") as f:\n",
    "        f.write(\"filter_number = {}\\n\".format(filter_number))\n",
    "        f.write(\"filter_size = {}\\n\".format(filter_size))\n",
    "        f.write(\"file_name = {}\\n\".format(file_name))\n",
    "        f.write(\"modulate = {}\\n\".format('bpsk'))\n",
    "        f.write(\"epoch = {}\\n\".format(epoch_read))\n",
    "        if is_turn:\n",
    "            f.write(\"turn = {}\\n\".format(turn))\n",
    "        f.write(\"training_acc = {}\\n\".format(training_acc))\n",
    "        f.write(\"validation_acc = {}\\n\".format(validation_acc))\n",
    "        f.write(validation_report)\n",
    "\n",
    "\n",
    "get_acc(epoch)\n",
    "\n",
    "# n = int(epoch / 100)\n",
    "# for e in range(n):\n",
    "#     epoch_read = (e + 1) * 100\n",
    "#     model.load(\"model-channel-coding-{}\".format(epoch_read * 20))\n",
    "#     get_acc(model, epoch_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
